{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import math\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tokenizers\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect hardware, return appropriate distribution strategy (you can see that it is pretty easy to set up).\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is set (always set in Kaggle)\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "MAX_LEN = 256\n",
    "ARTIFACTS_PATH = '../artifacts/'\n",
    "\n",
    "BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
    "EPOCHS = 10\n",
    "\n",
    "if not os.path.exists(ARTIFACTS_PATH):\n",
    "    os.makedirs(ARTIFACTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Post_Data/Training_data.csv')\n",
    "\n",
    "X_data = df[['text']].to_numpy().reshape(-1)\n",
    "y_data = df[['label']].to_numpy().reshape(-1)\n",
    "\n",
    "categories = df.values.reshape(-1)\n",
    "\n",
    "counter_categories = Counter(categories)\n",
    "category_names = counter_categories.keys()\n",
    "category_values = counter_categories.values()\n",
    "\n",
    "y_pos = np.arange(len(category_names))\n",
    "\n",
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.bar(y_pos, category_values, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, category_names)\n",
    "plt.ylabel('Number of texts')\n",
    "plt.title('Distribution of texts per category')\n",
    "plt.gca().yaxis.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(counter_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(df, split_char=' '):\n",
    "    categories = df['label'].unique()\n",
    "    \n",
    "    all_lengths = []\n",
    "    per_category = {\n",
    "        'lengths': {c:[] for c in categories},\n",
    "        'mean': {c:0 for c in categories},\n",
    "        'stdev': {c:0 for c in categories}\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        text = re.sub(r\"\\s+\", ' ', text) # Normalize\n",
    "        text = text.split(split_char)\n",
    "        l = len(text)\n",
    "        \n",
    "        category = row['label']\n",
    "        \n",
    "        all_lengths.append(l)\n",
    "        per_category['lengths'][category].append(l)\n",
    "    \n",
    "    for c in categories:\n",
    "        per_category['mean'][c] = statistics.mean(per_category['lengths'][c])\n",
    "        per_category['stdev'][c] = statistics.stdev(per_category['lengths'][c])\n",
    "    \n",
    "    global_stats = {\n",
    "        'mean': statistics.mean(all_lengths),\n",
    "        'stdev': statistics.stdev(all_lengths),\n",
    "        'lengths': all_lengths\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'global': global_stats,\n",
    "        'per_category': pd.DataFrame(per_category)\n",
    "    }\n",
    "\n",
    "\n",
    "def display_lengths_histograms(df_stats, n_cols=3):\n",
    "    categories = df['label'].unique()\n",
    "    n_rows = math.ceil(len(categories) / n_cols)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.suptitle('Distribution of lengths')\n",
    "    \n",
    "    # Subplot of all lengths\n",
    "    plt.subplot(n_rows, n_cols, 1)\n",
    "    plt.title('All categories')\n",
    "    lengths = df_stats['global']['lengths']\n",
    "    plt.hist(lengths, color='r')\n",
    "\n",
    "    # Subplot of each category\n",
    "    index_subplot = 2\n",
    "    for c in categories:\n",
    "        plt.subplot(n_rows, n_cols, index_subplot)\n",
    "        plt.title('label: %s' % c)\n",
    "        \n",
    "        lengths = df_stats['per_category']['lengths'][c]\n",
    "        plt.hist(lengths, color='b')\n",
    "\n",
    "        index_subplot += 1\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = calculate_stats(df)\n",
    "df_stats['per_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_lengths_histograms(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_texts = len(X_data)\n",
    "print('Texts in dataset: %d' % n_texts)\n",
    "\n",
    "categories = df['label'].unique()\n",
    "n_categories = len(categories)\n",
    "print('Number of categories: %d' % n_categories)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_encode(texts, tokenizer):\n",
    "    ct = len(texts)\n",
    "    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n",
    "    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n",
    "    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32') # Not used in text classification\n",
    "\n",
    "    for k, text in enumerate(texts):\n",
    "        # Tokenize\n",
    "        tok_text = tokenizer.tokenize(text)\n",
    "        \n",
    "        # Truncate and convert tokens to numerical IDs\n",
    "        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])\n",
    "        \n",
    "        input_length = len(enc_text) + 2\n",
    "        input_length = input_length if input_length < MAX_LEN else MAX_LEN\n",
    "        \n",
    "        # Add tokens [CLS] and [SEP] at the beginning and the end\n",
    "        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n",
    "        \n",
    "        # Set to 1s in the attention input\n",
    "        attention_mask[k,:input_length] = 1\n",
    "\n",
    "    return {\n",
    "        'input_word_ids': input_ids,\n",
    "        'input_mask': attention_mask,\n",
    "        'input_type_ids': token_type_ids\n",
    "    }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform categories into numbers\n",
    "category_to_id = {}\n",
    "category_to_name = {}\n",
    "\n",
    "for index, c in enumerate(y_data):\n",
    "    if c in category_to_id:\n",
    "        category_id = category_to_id[c]\n",
    "    else:\n",
    "        category_id = len(category_to_id)\n",
    "        category_to_id[c] = category_id\n",
    "        category_to_name[category_id] = c\n",
    "    \n",
    "    y_data[index] = category_id\n",
    "\n",
    "# Display dictionary\n",
    "category_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=777) # random_state to reproduce results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tokenizer from HuggingFace\n",
    "from transformers import CamembertForMaskedLM\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = roberta_encode(X_train, tokenizer)\n",
    "X_test = roberta_encode(X_test, tokenizer)\n",
    "\n",
    "y_train = np.asarray(y_train, dtype='int32')\n",
    "y_test = np.asarray(y_test, dtype='int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_categories):\n",
    "    with strategy.scope():\n",
    "        input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n",
    "        input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n",
    "        input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n",
    "\n",
    "        # Import RoBERTa model from HuggingFace\n",
    "        roberta_model = TFRobertaModel.from_pretrained(MODEL_NAME)\n",
    "        x = roberta_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n",
    "\n",
    "        # Huggingface transformers have multiple outputs, embeddings are the first one,\n",
    "        # so let's slice out the first position\n",
    "        x = x[0]\n",
    "\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dense(n_categories, activation='softmax')(x)\n",
    "\n",
    "        model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = build_model(n_categories)\n",
    "    model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    print('Training...')\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=EPOCHS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        verbose=1,\n",
    "                        validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot will look much better if we train models with more epochs, but anyway here is\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracy')\n",
    "\n",
    "xaxis = np.arange(len(history.history['accuracy']))\n",
    "plt.plot(xaxis, history.history['accuracy'], label='Train set')\n",
    "plt.plot(xaxis, history.history['val_accuracy'], label='Validation set')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(X_test, y_test, model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = [np.argmax(i) for i in model.predict(X_test)]\n",
    "\n",
    "    con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred).numpy()\n",
    "\n",
    "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    label_names = list(range(len(con_mat_norm)))\n",
    "\n",
    "    con_mat_df = pd.DataFrame(con_mat_norm,\n",
    "                              index=label_names, \n",
    "                              columns=label_names)\n",
    "\n",
    "    figure = plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(con_mat_df, cmap=plt.cm.Blues, annot=True)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = clean_text(text)\n",
    "encodings = tokenizer([text], max_length=200, truncation=True, padding=True)\n",
    "ds = tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "predictions = model.predict(ds)\n",
    "\n",
    "import numpy as np\n",
    "print(mapping[np.argmax(predictions[0])])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('CamemBERT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "486b0e0edd4f9c3deb84fbdbb1a71a0d22dc9a3de96515e081bea437ec89d2f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
